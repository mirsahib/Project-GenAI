{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "134efc2f-7ec8-4a93-803d-d1a120882557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /home/mirsahib/miniconda3/envs/nlp/lib/python3.12/site-packages (from nltk) (4.66.5)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (797 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.0/797.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.9.1 regex-2024.9.11\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b35f1ba-8bf4-47fc-868c-759062a01f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3e50632-41df-4a64-bdec-7112bfaff79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/mirsahib/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')  # This will download the tokenizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32a0a734-9fc6-4b4d-a9fc-55c1e52fb907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'fascinating', '!']\n"
     ]
    }
   ],
   "source": [
    "#word tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Natural Language Processing (NLP) is fascinating!\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a437eece-6997-41c4-a8ae-d47dee9ad9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP is an exciting field.', 'It involves understanding human language.']\n"
     ]
    }
   ],
   "source": [
    "#sentence tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Example text\n",
    "text = \"NLP is an exciting field. It involves understanding human language.\"\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09e31838-de68-4e48-ad96-414a952f6d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'important', 'Let', 's', 'break', 'this', 'text', 'into', 'words']\n"
     ]
    }
   ],
   "source": [
    "#Regexp Tokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# Create a custom tokenizer that extracts only words (ignoring punctuation)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Example text\n",
    "text = \"Tokenization is important! Let's break this text into words.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a39e7ffe-7574-49f8-9c92-167b8ceb8810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['simple', 'example', 'demonstrating', 'removal', 'stopwords', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mirsahib/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#Tokenization with Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if you haven't\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Example text\n",
    "text = \"This is a simple example demonstrating the removal of stopwords.\"\n",
    "\n",
    "# Tokenize the text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Filter out stop words\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "print(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be273608-5034-4b07-aadd-f8887c07b890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/mirsahib/miniconda3/envs/nlp/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/mirsahib/miniconda3/envs/nlp/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:06\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ced97e79-00f0-49b6-b6f8-40536ef7208a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'the': 8, 'cat': 0, 'sat': 7, 'on': 6, 'mat': 4, 'dog': 2, 'log': 3, 'chased': 1, 'mouse': 5}\n",
      "Bag of Words Matrix:\n",
      " [[1 0 0 0 1 0 1 1 2]\n",
      " [0 0 1 1 0 0 1 1 2]\n",
      " [1 1 0 0 0 1 0 0 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample corpus (list of sentences)\n",
    "corpus = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog sat on the log.\",\n",
    "    \"The cat chased the mouse.\"\n",
    "]\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the model and transform the corpus into a bag of words representation\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Display the vocabulary (unique words in the corpus)\n",
    "print(\"Vocabulary:\", vectorizer.vocabulary_)\n",
    "\n",
    "# Display the bag of words matrix\n",
    "print(\"Bag of Words Matrix:\\n\", X.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "580091c1-080b-41f6-a003-889b3fa62409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: ['cat' 'chased' 'dog' 'in' 'is' 'log' 'mat' 'mouse' 'on' 'the']\n",
      "TF-IDF Matrix:\n",
      " [[0.3564574  0.         0.         0.         0.3564574  0.\n",
      "  0.46869865 0.         0.46869865 0.55364194]\n",
      " [0.         0.         0.44839402 0.44839402 0.34101521 0.44839402\n",
      "  0.         0.         0.         0.52965746]\n",
      " [0.38151877 0.50165133 0.         0.         0.         0.\n",
      "  0.         0.50165133 0.         0.59256672]]\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample corpus of documents\n",
    "corpus = [\n",
    "    \"The cat is on the mat.\",\n",
    "    \"The dog is in the log.\",\n",
    "    \"The cat chased the mouse.\"\n",
    "]\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the model and transform the corpus into TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Display the feature names (words) and their TF-IDF scores\n",
    "print(\"Feature Names:\", vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cda826bc-7b40-4966-9ea2-e190285d4c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: ['are fascinating' 'language models' 'language processing' 'love natural'\n",
      " 'models are' 'natural language']\n",
      "Bigram Frequency Matrix:\n",
      " [[0 0 1 1 0 1]\n",
      " [1 1 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "#n-Grams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample text corpus\n",
    "corpus = [\n",
    "    \"I love natural language processing.\",\n",
    "    \"Language models are fascinating.\"\n",
    "]\n",
    "\n",
    "# Initialize the CountVectorizer with n-gram range (bigrams in this case)\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))  # Bigram model\n",
    "\n",
    "# Fit and transform the corpus to get bigrams\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (bigrams) and display the bigram counts\n",
    "print(\"Bigrams:\", vectorizer.get_feature_names_out())\n",
    "print(\"Bigram Frequency Matrix:\\n\", X.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edec498b-5274-4adc-95a4-e163d296e978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
